import re
from typing import List, Dict

class MedicalSession:
    def __init__(self):
        self.questions_asked = 0
        self.finished = False
        self.last_bot_message = "ØªÙ…Ø§Ù… Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ØŒ ØªÙ‚Ø¯Ø± ØªÙ‚ÙˆÙ„ÙŠ Ø¥ÙŠÙ‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©ØŸ"
        self.useless_responses = {
            "Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù‡Ù„Ø§", "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ù‡Ø§ÙŠ", "Ø¥Ø²ÙŠÙƒ", 
            "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±", "Ù‡Ù„Ø§", "Ù‡Ø§ÙŠ", "ÙƒÙŠÙÙƒ"
        }
        self.messages = [
            {
                "role": "system",
                "content": "\n".join([
                    "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø·Ø¨ÙŠ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø±Ø¶Ù‰ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ ÙÙŠ Ø§Ù„ØªØ´Ø®ÙŠØµ.",
                    "Ø³ÙŠØªÙ… ØªØ²ÙˆÙŠØ¯Ùƒ Ø¨Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£ÙˆÙ„ÙŠØ© Ø¹Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø±ÙŠØ¶.",
                    "Ù…Ù‡Ù…ØªÙƒ Ø¥Ù†Ùƒ ØªØ³Ø£Ù„ Ø§Ù„Ù…Ø±ÙŠØ¶ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ¯ÙˆØ¯Ø© ÙˆØ·Ø¨ÙŠØ¹ÙŠØ© Ø²ÙŠ Ø¯ÙƒØªÙˆØ± Ø¬Ù„Ø¯ÙŠØ© Ø¨ÙŠØ³Ø£Ù„ Ù…Ø±ÙŠØ¶Ù‡.",
                    # ... [rest of the system prompt from notebook]
                    "Ø±Ø¯ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø³"
                ])
            }
        ]

    def process_input(self, user_input: str, tokenizer, model, device: str) -> str:
        user_input_cleaned = user_input.strip().lower()

        if self.finished or re.match(r"^\\s*ØªÙ…Ø§Ù…\\b", user_input_cleaned, re.IGNORECASE) or \
           user_input_cleaned in ["Ù„Ø§ Ø´ÙƒØ±Ø§", "Ø®Ø±ÙˆØ¬", "Ø§Ù†ØªÙ‡Ø§Ø¡", "exit", "quit"]:
            self.finished = True
            return "Ø´ÙƒØ±Ù‹Ø§ Ù„Ùƒ! Ø£ØªÙ…Ù†Ù‰ Ù„Ùƒ Ø§Ù„ØµØ­Ø© ÙˆØ§Ù„Ø¹Ø§ÙÙŠØ©. ðŸ˜Š"

        self.messages.append({"role": "user", "content": user_input})

        if user_input_cleaned in self.useless_responses:
            response = self.last_bot_message
            self.messages.append({"role": "assistant", "content": response})
            return response

        text = tokenizer.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text], return_tensors="pt").to(device)
        generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=100, do_sample=False)

        generated_ids = [
            output_ids[len(input_ids):]
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()

        self.last_bot_message = response
        self.messages.append({"role": "assistant", "content": response})
        self.questions_asked += 1

        return response

    def get_full_arabic_conversation(self) -> str:
        return "\n".join([
            f"{'Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯' if msg['role'] == 'assistant' else 'Ø§Ù„Ù…Ø±ÙŠØ¶'}: {msg['content']}"
            for msg in self.messages if msg["role"] in {"user", "assistant"}
        ])